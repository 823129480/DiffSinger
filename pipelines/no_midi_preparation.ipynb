{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Prepare your own dataset for DiffSinger (MIDI-less version)\n",
    "\n",
    "## 1 Overview\n",
    "\n",
    "This Jupyter Notebook will guide you to prepare your own dataset for DiffSinger with 44.1 kHz sampling rate.\n",
    "Please read and follow the guidance carefully, take actions when there are notice for <font color=\"red\">manual action</font> and pay attention to blocks marked with <font color=\"red\">optional step</font>.\n",
    "\n",
    "### 1.1 Introduction to this pipeline and MIDI-less version\n",
    "\n",
    "This pipeline does not support customized phoneme dictionaries. It uses the [opencpop strict pinyin dictionary](../dictionaries/opencpop-strict.txt) by default.\n",
    "\n",
    "MIDI-less version is a simplified version of DiffSinger where MIDI layers, word layers and slur layers are removed from the data labels. The model uses raw phoneme sequence with durations as input, and applies pitch embedding directly from the ground truth. Predictors for phoneme durations and pitch curve are also removed. Below are some limitations and advantages of the MIDI-less version:\n",
    "\n",
    "- The model will not predict phoneme durations and f0 sequence by itself. You must specify `ph_dur` and `f0_seq` at inference time.\n",
    "- Performance of pitch control will be better than MIDI-A version, because MIDI keys are misleading information for the diffusion decoder when f0 sequence is already embedded.\n",
    "- MIDIs and slurs does not need to be labeled, thus the labeling work is easier than other versions.\n",
    "- More varieties of data can be used as training materials, even including speech.\n",
    "\n",
    "### 1.2 Install dependencies\n",
    "\n",
    "Please run the following code block the first time you start this notebook.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install librosa soundfile matplotlib\n",
    "!conda install -c conda-forge montreal-forced-aligner\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Initializing environment\n",
    "\n",
    "Please run the following code block every time you start this notebook.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import wave\n",
    "\n",
    "import librosa\n",
    "import soundfile\n",
    "\n",
    "\n",
    "def length(src: str):\n",
    "    if os.path.isfile(src) and src.endswith('.wav'):\n",
    "        with wave.open(src, 'r') as w:\n",
    "            return w.getnframes() / w.getframerate() / 3600\n",
    "    elif os.path.isdir(src):\n",
    "        total = 0\n",
    "        for ch in [os.path.join(src, c) for c in os.listdir(src)]:\n",
    "            total += length(ch)\n",
    "        return total\n",
    "    return 0\n",
    "\n",
    "\n",
    "print('Environment initialized successfully.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Raw recordings and audio slicing\n",
    "\n",
    "### 2.1 Choose raw recordings\n",
    "\n",
    "Your recordings must meet the following conditions:\n",
    "\n",
    "1. They must be in one single folder. Files in sub-folders will be ignored.\n",
    "2. They must be in WAV format.\n",
    "3. They must have a sampling rate higher than 32 kHz.\n",
    "4. They should contain only voices from human, and only one human, since multi-speaker training is not supported yet.\n",
    "5. They should be clean voices with no significant noise or reverb.\n",
    "\n",
    "<font color=\"red\">Optional step</font>: The raw data must be sliced into parts of about 5-15 seconds. If you want to do this yourself, please skip to section 2.3. Otherwise, please edit paths in the following code block before you run it.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "# Configuration for data paths\n",
    "raw_path = 'path/to/your/raw/recordings'  # Path to your raw, unsliced recordings\n",
    "\n",
    "########################################\n",
    "\n",
    "assert os.path.exists(raw_path) and os.path.isdir(raw_path), 'The chosen path does not exist or is not a directory.'\n",
    "print('Raw recording path:', raw_path)\n",
    "print()\n",
    "print('===== Recording List =====')\n",
    "raw_filelist = glob.glob(f'{raw_path}/*.wav', recursive=True)\n",
    "raw_length = length(raw_path)\n",
    "if len(raw_filelist) > 5:\n",
    "    print('\\n'.join(raw_filelist[:5] + [f'... ({len(raw_filelist) - 5} more)']))\n",
    "else:\n",
    "    print('\\n'.join(raw_filelist))\n",
    "print()\n",
    "print(f'Found {len(raw_filelist)} valid recordings with total length of {round(raw_length, 2)} hours.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Audio slicing\n",
    "\n",
    "We provide an audio slicer which automatically cuts recordings into short pieces.\n",
    "\n",
    "The audio slicer is based on silence detection and has several arguments that have to be specified. You should modify these arguments according to your data.\n",
    "\n",
    "For more details of each argument, see its [GitHub repository](https://github.com/openvpi/audio-slicer).\n",
    "\n",
    "Please edit paths and arguments in the following code block before you run it.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "# Configuration for data paths\n",
    "sliced_path = 'path/to/your/sliced/recordings'  # Path to hold the sliced segments of your recordings\n",
    "\n",
    "# Slicer arguments\n",
    "db_threshold_ = -40.\n",
    "min_length_ = 5000\n",
    "win_l_ = 400\n",
    "win_s_ = 20\n",
    "max_silence_kept_ = 500\n",
    "\n",
    "# Number of threads (based on your CPU kernels)\n",
    "num_workers = 5\n",
    "\n",
    "########################################\n",
    "\n",
    "assert 'raw_path' in locals().keys(), 'Raw path of your recordings has not been specified.'\n",
    "assert not os.path.exists(sliced_path) or os.path.isdir(sliced_path), 'The chosen path is not a directory.'\n",
    "os.makedirs(sliced_path, exist_ok=True)\n",
    "print('Sliced recording path:', sliced_path)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED\n",
    "\n",
    "from utils.slicer import Slicer\n",
    "\n",
    "\n",
    "def slice_one(in_audio):\n",
    "    audio, sr = librosa.load(in_audio, sr=None)\n",
    "    slicer = Slicer(\n",
    "        sr=sr,\n",
    "        db_threshold=db_threshold_,\n",
    "        min_length=min_length_,\n",
    "        win_l=win_l_,\n",
    "        win_s=win_s_,\n",
    "        max_silence_kept=max_silence_kept_\n",
    "    )\n",
    "    chunks = slicer.slice(audio)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        soundfile.write(os.path.join(sliced_path, f'%s_slice_%04d.wav' % (os.path.basename(in_audio).rsplit('.', maxsplit=1)[0], i)), chunk, sr)\n",
    "\n",
    "\n",
    "print('Slicing your recordings may take several minutes. Please wait.')\n",
    "thread_pool = ThreadPoolExecutor(max_workers=num_workers)\n",
    "tasks = []\n",
    "for file in raw_filelist:\n",
    "    tasks.append(thread_pool.submit(slice_one, file))\n",
    "wait(tasks, return_when=ALL_COMPLETED)\n",
    "print()\n",
    "print('===== Segment List =====')\n",
    "sliced_filelist = glob.glob(f'{sliced_path}/*.wav', recursive=True)\n",
    "sliced_length = length(sliced_path)\n",
    "if len(sliced_filelist) > 5:\n",
    "    print('\\n'.join(sliced_filelist[:5] + [f'... ({len(sliced_filelist) - 5} more)']))\n",
    "else:\n",
    "    print('\\n'.join(sliced_filelist))\n",
    "print()\n",
    "print(f'Sliced your recordings into {len(sliced_filelist)} segments with total length of {round(sliced_length, 2)} hours.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Validating recording segments\n",
    "\n",
    "In this section, we validate your recording segments.\n",
    "\n",
    "<font color=\"red\">Optional step</font>: If you skipped section 2.2, please specify the path to your sliced recordings in the following code block and run it. Otherwise, skip this code block.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################\n",
    "\n",
    "# Configuration for data paths\n",
    "sliced_path_ = 'assets'  # r'D:\\Vocoder Datasets\\formatted\\44100\\00'  # 'path/to/your/sliced/recordings'  # Path to your sliced segments of recordings\n",
    "\n",
    "########################################\n",
    "\n",
    "if not 'sliced_path' in locals().keys():\n",
    "    sliced_path = sliced_path_\n",
    "    assert os.path.exists(sliced_path) and os.path.isdir(sliced_path), 'The chosen path does not exist or is not a directory.'\n",
    "\n",
    "print('Sliced recording path:', sliced_path)\n",
    "print()\n",
    "print('===== Segment List =====')\n",
    "sliced_filelist = glob.glob(f'{sliced_path}/*.wav', recursive=True)\n",
    "sliced_length = length(sliced_path)\n",
    "if len(sliced_filelist) > 5:\n",
    "    print('\\n'.join(sliced_filelist[:5] + [f'... ({len(sliced_filelist) - 5} more)']))\n",
    "else:\n",
    "    print('\\n'.join(sliced_filelist))\n",
    "print()\n",
    "print(f'Found {len(sliced_filelist)} valid segments with total length of {round(sliced_length, 2)} hours.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following code block to check if there are segments with an unexpected length (less than 2 seconds or more than 30 seconds).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reported = False\n",
    "for file in sliced_filelist:\n",
    "    with wave.open(file, 'r') as wav:\n",
    "        wave_seconds = wav.getnframes() / wav.getframerate()\n",
    "        if wave_seconds < 2.:\n",
    "            reported = True\n",
    "            print(f'Too short! \\'{file}\\' has a length of {round(wave_seconds, 1)} seconds!')\n",
    "        if wave_seconds > 30.:\n",
    "            reported = True\n",
    "            print(f'Too long! \\'{file}\\' has a length of {round(wave_seconds, 1)} seconds!')\n",
    "if not reported:\n",
    "    print('Congratulations! All segments have proper length.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">Manual action</font>: please consider removing segments too short and manually slicing segments to long, as reported above.\n",
    "\n",
    "Move on when this is done or there are no segments reported.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Label your segments\n",
    "\n",
    "### 3.1 Label syllable sequence\n",
    "\n",
    "All segments should have their transcriptions (or lyrics) annotated. Run the following code block to see the example segment (from Opencpop dataset) and its corresponding annotation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# noinspection PyTypeChecker\n",
    "display(Audio(filename='assets/2001000001.wav'))\n",
    "with open('assets/2001000001.lab', 'r') as f:\n",
    "    print(f.read())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">Manual action</font>: now your task is to annotation transcriptions for each segment like the example shown above.\n",
    "\n",
    "Each segment should have one annotation file with the same filename as it and `.lab` extension, and placed in the same directory. In the annotation file, you should write all syllables sung or spoken in this segment. Syllables should be split by space, and only syllables that appears in the dictionary are allowed. In addition, all phonemes in the dictionary should be covered in the annotations.\n",
    "\n",
    "**Special notes**: `AP` and `SP` should not appear in the annotation.\n",
    "\n",
    "**News**:  We developed [MinLabel](https://github.com/SineStriker/qsynthesis-revenge/tree/main/src/Test/MinLabel), a simple yet efficient tool to help finishing this step. You can download the binary executable for Windows [here](https://diffsinger-1307911855.cos.ap-beijing.myqcloud.com/label/minlabel_latest.zip).\n",
    "\n",
    "<font color=\"red\">Optional step</font>: if you want us to help you create all empty `lab` files (instead of creating them yourself), please run the following code block.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in sliced_filelist:\n",
    "    filename = os.path.basename(file)\n",
    "    name_without_ext = filename.rsplit('.', maxsplit=1)[0]\n",
    "    annotation = os.path.join(sliced_path, f'{name_without_ext}.lab')\n",
    "    if not os.path.exists(annotation):\n",
    "        with open(annotation, 'a'):\n",
    "            print(f'Created: \\'{annotation}\\'')\n",
    "print('Creating missing lab files done.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following code block to see if all segments are annotated and all annotations are valid. If there are failed checks, please fix them and run again.\n",
    "\n",
    "A summary of your phoneme coverage will be generated. If there are some phonemes that have extremely few occurrences (for example, less than 20), it is highly recommended to add more recordings to cover these phonemes.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dictionary\n",
    "dict_path = '../dictionaries/opencpop-strict.txt'\n",
    "with open(dict_path, 'r', encoding='utf8') as f:\n",
    "    rules = [ln.strip().split('\\t') for ln in f.readlines()]\n",
    "dictionary = {}\n",
    "phoneme_set = set()\n",
    "for r in rules:\n",
    "    phonemes = r[1].split()\n",
    "    dictionary[r[0]] = phonemes\n",
    "    phoneme_set.update(phonemes)\n",
    "\n",
    "# Run checks\n",
    "check_failed = False\n",
    "covered = set()\n",
    "phoneme_map = {}\n",
    "for ph in sorted(phoneme_set):\n",
    "    phoneme_map[ph] = 0\n",
    "\n",
    "segment_pairs = []\n",
    "\n",
    "for file in sliced_filelist:\n",
    "    filename = os.path.basename(file)\n",
    "    name_without_ext = filename.rsplit('.', maxsplit=1)[0]\n",
    "    annotation = os.path.join(sliced_path, f'{name_without_ext}.lab')\n",
    "    if not os.path.exists(annotation):\n",
    "        print(f'No annotation found for \\'{filename}\\'!')\n",
    "        check_failed = True\n",
    "    with open(annotation, 'r', encoding='utf8') as f:\n",
    "        syllables = f.read().strip().split()\n",
    "    if not syllables:\n",
    "        print(f'Annotation file \\'{annotation}\\' is empty!')\n",
    "        check_failed = True\n",
    "    else:\n",
    "        oov = []\n",
    "        for s in syllables:\n",
    "            if s not in dictionary:\n",
    "                oov.append(s)\n",
    "            else:\n",
    "                for ph in dictionary[s]:\n",
    "                    phoneme_map[ph] += 1\n",
    "                covered.update(dictionary[s])\n",
    "        if oov:\n",
    "            print(f'Syllable(s) {oov} not allowed in annotation file \\'{annotation}\\'')\n",
    "            check_failed = True\n",
    "\n",
    "# Phoneme coverage\n",
    "uncovered = phoneme_set - covered\n",
    "if uncovered:\n",
    "    print(f'The following phonemes are not covered!')\n",
    "    print(sorted(uncovered))\n",
    "    print('Please add more recordings to cover these phonemes.')\n",
    "    check_failed = True\n",
    "\n",
    "if not check_failed:\n",
    "    print('Congratulations! All annotations are well prepared.')\n",
    "    print('Here are a summary of your phoneme coverage.')\n",
    "\n",
    "fig = plt.figure(figsize=(int(len(phoneme_set) * 0.8), 10))\n",
    "x = list(phoneme_map.keys())\n",
    "values = list(phoneme_map.values())\n",
    "plt.bar(x=x, height=values)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.xlim(-1, len(phoneme_set))\n",
    "for a, b in zip(x, values):\n",
    "    plt.text(a, b, b, ha='center', va='bottom', fontsize=15)\n",
    "plt.grid()\n",
    "plt.title('Phoneme Distribution Summary', fontsize=30)\n",
    "plt.xlabel('Phoneme', fontsize=20)\n",
    "plt.ylabel('Number of occurrences', fontsize=20)\n",
    "\n",
    "phoneme_summary = os.path.join(sliced_path, 'phoneme_distribution.jpg')\n",
    "plt.savefig(fname=phoneme_summary,\n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.25)\n",
    "plt.show()\n",
    "print(f'Summary saved to \\'{phoneme_summary}\\'.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Forced alignment\n",
    "\n",
    "Given the transcriptions of each segment, we are able to align the phoneme sequence to its corresponding audio, thus obtaining position and duration information of each phoneme.\n",
    "\n",
    "We use [Montreal Forced Aligner](https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner) to do forced phoneme alignment.\n",
    "\n",
    "Run the following code block to download and unzip the pretrained MFA acoustic model.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "mfa_dirname = 'assets/mfa-opencpop-strict'\n",
    "mfa_zip = f'{mfa_dirname}.zip'\n",
    "mfa_uri = 'https://diffsinger-1307911855.cos.ap-beijing.myqcloud.com/mfa/mfa-opencpop-strict.zip'\n",
    "if not os.path.exists(mfa_dirname):\n",
    "    # Download\n",
    "    print('Model not found, downloading...')\n",
    "    with open(mfa_zip, 'wb') as f:\n",
    "        f.write(requests.get(mfa_uri).content)\n",
    "    # Unzip\n",
    "    print('Unzipping...')\n",
    "    with zipfile.ZipFile(mfa_zip, 'r') as zf:\n",
    "        zf.extractall(path='assets/')\n",
    "    # Clean\n",
    "    print('Cleaning...')\n",
    "    os.remove(mfa_zip)\n",
    "    print('Done.')\n",
    "else:\n",
    "    print('Model already exists. Please move on.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run MFA alignment, please first run the following code block to resample all recordings to 16 kHz.\n",
    "\n",
    "The resampled recordings will be saved, and the phoneme labels will be copied, at `./segments/`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segments_dir = 'segments'\n",
    "\n",
    "if os.path.exists(segments_dir):\n",
    "    shutil.rmtree(segments_dir)\n",
    "os.makedirs(segments_dir)\n",
    "for file in sliced_filelist:\n",
    "    samplerate = 16000\n",
    "    y, _ = librosa.load(file, sr=samplerate, mono=True)\n",
    "    filename = os.path.basename(file)\n",
    "    soundfile.write(os.path.join(segments_dir, filename), y, samplerate, subtype='PCM_16')\n",
    "    name_without_ext = filename.rsplit('.', maxsplit=1)[0]\n",
    "    annotation = os.path.join(sliced_path, f'{name_without_ext}.lab')\n",
    "    shutil.copy(annotation, segments_dir)\n",
    "print('Resampling and copying done.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Row run the following code block to run forced alignment.\n",
    "\n",
    "The results will be saved at `./textgrids`.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "textgrids_dir = 'textgrids'\n",
    "!mfa align $segments_dir $dict_path $mfa_dirname $textgrids_dir --beam 100 --clean --overwrite\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
